---
title: "Inverse Probability Weighting for Time Dependent Data"
author: "Ashley Naimi"
date:
header-includes:
   - \usepackage[default]{sourcesanspro}
   - \usepackage[T1]{fontenc}
mainfont: SourceSansPro
output: html_document
bibliography: ref_main_v4.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this document, we will cover methods to deal with complex longitudinal data. The example data are simulated from an algorithm developed by Jessica Young [@Young2010] and later adapted by Erica Moodie [@Moodie2014]. 

We will start by loading some relevant packages in R:

```{r load_data, warning=F,message=F}

# install and load packages
packages <- c("data.table","tidyverse","skimr",
              "here","ggthemes","extrafont","survival",
              "sandwich","lmtest")

for (package in packages) {
  if (!require(package, character.only=T, quietly=T)) {
    install.packages(package, repos='http://lib.stat.cmu.edu/R/CRAN')
  }
}

for (package in packages) {
  library(package, character.only=T)
}

font_import(pattern = 'Arial')

# make fancy pants figures
thm <- theme_tufte() +
  theme(
    text = element_text(family="Arial",size=12),
    legend.position = "top",
    legend.background = element_rect(fill = "transparent", colour = NA),
    legend.key = element_rect(fill = "transparent", colour = NA)
  )
theme_set(thm)

# options to load and examine data
## use one of these lines
D <- read_csv(here("data","example_dat.csv"))
# D <- read_csv("change path to directoty that includes data")
# D <- read.csv("change path to directoty that includes data")

D %>% print(n=16)

```

In these data, the sample consists of $N=5,000$ individuals, each with up to 12 time-points (e.g., weeks) of follow up. The total number of person-time units is `r nrow(D)`. 

The columns contain distinct variables: `int` is the time interval and `time` is the actual time. The only difference between these two is that, for the last observation for those who had an event, the `int` will be a integer whereas `time` will be a real number. 

The variables `X` and `Z` represent the time-dependent exposure and time-dependent confounder, respectively. `Xm1`, `Zm1`, and `timem1` represent the lagged versions of `X`, `Z`, `time`. That is, for a given row, the (for example) `Xm1` value will be the same as the value of `X` in the previous row (and zero if the previous row is pre time 0). Finally, `Y` is an indicator of whether the event occurred. 

Data like these (in long form), are sometimes referred to as the Andersen-Gill data structure, and are commonly encountered with survival data [@Therneau2000]. 

These data were generated from using a mechanism that can be described with the following DAG:
```{r,engine='tikz',echo=F,message=F,warning=F,fig.align="center"}
\begin{tikzpicture}
\node[align = center] at (0,3) (1) {$X_{j-1}$};
\node[align = center] at (3,3) (4) {$Z_{j}$};
\node[align = center] at (5.5,3) (2) {$X_{j}$};
\node[align = center] at (8,3) (3) {$Y$};
\node[align = center] at (5.5,1.5) (3) {$U$};
\draw[->, line width=.01cm] (1,3) -- (2.25,3);
\draw[->, line width=.01cm] (6,1.5) -- (7.65,2.25);
\draw[->, line width=.01cm] (5,1.5) -- (3,2.25);
\draw[->, line width=.01cm] (3.75,3) -- (4.9,3);
\draw[->, line width=.01cm] (6.25,3) -- (7.5,3);
\draw[->, line width=.01cm] (0.5,3.4) to [out=15,in=170] (7.5,3.4);
\draw[->, line width=.01cm] (3.5,2.5) to [out=345,in=200] (7.5,2.75);
\end{tikzpicture}
```

With these data, our interest lies in the average treatment effect. The dataset is generated such that we have a survival outcome, where `time` is the time to event, and `Y` is the event indicator. We can thus define the average treatment effect on the hazard ratio scale as:

$$\lambda_{T^{\overline{x}=1}}(t) / \lambda_{T^{\overline{x}=0}}(t)$$
which is interpreted as the hazard that would be observed if all individuals were exposed up to time $T = t$ divided by the hazard that would be observed if all individuals were unexposed up to time $T = t$. Of course, there are problems with hazard ratios [@Hernan2010]. To address these, we'd typically also (or exclusively) express interest in the causal risk function [@Cole2015], but we'll have to save that for another time.

# IP-Weighting

Our first objective will be to use IP-weighting in these data to "erase" the arrow from $Z_j$ to $X_j$, for all time points $j$. 

To do this, we first create two propensity scores. The first will be the usual propensity score, defined as the probability of being exposed at each time point, conditional on the lagged exposure (`Xm1`), the confounder (`Z`), and the lagged confounder (`Zm1`). This will serve as the denominator of our stabilized and unstabilized weights. The second is just the probability of being exposed at each time point. This will serve as the numerator of our stabilized weights.

```{r ip_weight, message=F,warning=F}
# propensity score
D$pscore <- glm(X ~ as.factor(int) + Xm1 + Z + Zm1,data=D,family=binomial(link="logit"))$fitted.values

# numerator of stabilized weights
D$p_num <- glm(X ~ as.factor(int),data=D,family=binomial(link="logit"))$fitted.values
```

Let's take a look at the dataset now that it has the propensity score:
```{r}
D %>% print(n=16)
```

Next, we create the stabilized and unstabilzed weights. To do this, we first obtain the probability of the observed exposure. That is, in a given time point, if a person is exposed, their numerator should be `p_num` and their denominator should be `pscore`. If they are instead unexposed, their numerator should be `1 - p_num` and their denominator should be `1 - pscore`:
```{r}
D <- D %>% group_by(ID) %>% 
  mutate(num = X*p_num + (1-X)*(1-p_num),
         den = X*pscore + (1-X)*(1-pscore),
         sw = cumprod(num/den),
         w = cumprod(1/den)) %>% 
  ungroup(ID)
```

```{r}
D %>% select(ID, time, X, pscore, p_num, num, den, sw, w) %>% print(n=16)
```

A quick note on the difference between stabilized and unstabilized weights. In a weighted analysis, the sample size contribution of each row is increased (or decreased) by its weight. With unstabilized weights, the sample size contribution for a given row can quickly outweight the sample. For example, the unstablized weight for the last observation for ID = 1 is `r D %>% filter(ID==1,int==12) %>% select(w) %>% round(.,0)`. This is nearly 80% of the sample size of 5,000! In contrast, the stabilized weight for the sample observation and row is `r D %>% filter(ID==1,int==12) %>% select(sw) %>% round(.,2)`.

Because of this problem with unstablized weights, the general recommendation is to use always stabilized weights.

With our weights created, the next step is to evaluate the distribution of the weights and the propensity score. This step is important because it's one of the ways in which we can evaluate whether **positivity** holds. Recall that positivity requires that there be exposed and unexposed individuals within all confounder strata at all time points. Expressed mathematically, positivity requires that the probability of being exposed conditional on all confounders is bounded away from zero and one for all individuals over all time points:

$$0 < P(X_j = 1 \mid X_{j-1}, Z_j, Z_{j-1}) < 1, \forall j.$$
Why is this important? Suppose that for a certain exposed individual $i$, the probability of being exposed is zero. When we create the weights for this person, we end up with an expression that looks like this:
$$w_i = \frac{1}{P(X_j = 1 \mid X_{j-1}, Z_j, Z_{j-1})} = \frac{1}{0},$$
which is undefined. In fact, because of this, the conditional probability of being exposed can't even be close to zero. If, for example, this probability is $0.0001$, then the weight becomes:
$$w_i = \frac{1}{P(X_j = 1 \mid X_{j-1}, Z_j, Z_{j-1})} = \frac{1}{0.0001} = 10,000.$$
This means that in a dataset like ours, with 5000 individuals, one individual's person-time contribution will be counted 10,000 times, which will have an overwhelming impact on our estimate of the causal effect.

There are a few techniques we can use to evaluate whether such problematic weights exist. As a first step, we can simply look at the distribution of the propensity score:
```{r, warning=F, message=F,fig.align="center"}
ggplot(D) + 
  geom_histogram(aes(pscore)) +
  xlab("Propensity Score") +
  ylab("Count")
```

This doesn't reveal any immediate concerns, but there are two problems with it: 1) it is presenting propensity scores aggregated over all time points. This is a problem because the propensity score may be different at later time points than at earlier time points; 2) it is not conisdering the fact that what really matters is **propensity score overlap**, which tells us how comparable exposed and unexposed individuals are in our data. 

Two address these two problems, we can create a separate figure for each time point and, within each time point, look at how the propensity score distribution in the exposed group compares to the unexposed group:

```{r, warning=F, message=F,fig.align="center"}
ggplot(D) + 
  geom_density(aes(x=pscore,fill=as.factor(X)),alpha=.5,bw=.1) +
  facet_wrap(~int, labeller=label_both) +
  xlab("Propensity Score") +
  ylab("Density") +
  scale_fill_discrete(name = "Exposure Status", labels = c("Unexposed", "Exposed"))
```

This figure reveals a little more. Namely, the overlap separates slightly over time on study. But the degree of separation is not indicative of any problems.

The final strategy that we'll use to evaluate the weights is to check their distribution, specifically the mean and the max of the **stabilized weights** at each time point:
```{r}
D %>% group_by(int) %>% summarize(meanSW = mean(sw),
                                  maxSW = max(sw))
```

The mean of the stabilized weights at all time points should be 1, and the max weight should not be large. In this case "large" is not formally defined, but a good rule-of-thumb is to compare the largest weight to the sample size. 

Overall, the distribution of the propensity score and weights is not suggestive of any positivity concerns.

Next, we use these weights to estimate the effect of interest. To estimate the causal hazard ratio, we can use a Cox proportional hazards regression model, or a pooled logistic regression model:


```{r}

mod1_cox <- coxph(Surv(time,Y) ~ X + cluster(ID), data=D, ties="efron", weight=sw)

summary(mod1_cox)$coefficients

mod1_plr <- glm(Y ~ as.factor(int) + X, data=D, family = quasibinomial(link = "logit"), weights=sw) 
# NB: quasibinomial is used instead of binomial to avoid a warning message. Both should yield the same numerical results!

coeftest(mod1_plr, vcov = vcovCL(mod1_plr, cluster=D$ID, type = "HC1"))[13,]

```

The Cox PH model yields a hazard ratio estimate of `r round(summary(mod1_cox)$coefficients[2],2)` and the pooled logistic model yields a hazard ratio estimate of `r round(exp(summary(mod1_plr)$coefficients[13,1]),2)`. To get appropriate standard errors for these estimates, we have to use the robust standard error estimator. This is accomplished using the `cluster(ID)` argument in the Cox model, and the `coeftest` function for the pooled logistic model. The robust standard error is `r round(summary(mod1_cox)$coefficients[4],2)` for the Cox model, and `r round(coeftest(mod1_plr, vcov = vcovCL(mod1_plr, cluster=D$ID, type = "HC1"))[13,2],2)` for the pooled logistic model.

## Lab Questions

1) You may want to use the code-snippet below to help you answer this. In the dataset `D`, how many individuals were actually exposed at all time-points over their follow-up? How many people were unexposed? What does this say about the extent to which the estimand (i.e., everyone exposed at all time points versus everyone unexposed at all time points) is supported by these data? 

```{r}
new_dat <- D %>% 
  group_by(ID) %>% 
  mutate(cumexp = cumsum(X),
         expratio = cumexp/int) %>% 
  filter(last_flag == 1)
```

2) Why do you think the maximum stabilized weight gets larger over time on study? What does this say about a study with only three follow-up time points versus 60 follow-up time points?

BONUS

3) What is the magnitude of the association between $Z_j$ and $X_j$ in the unweighted data? What happens to the magnitude of this association in the weighted data? (report coefficients and appropriate standard errors)

# g Computation

In our next example, we'll look at using g computation to analyze the same data. 

```{r}

```



# References